The ultimate goal of this project is to develop a mobile computing platform which utilizes an FPGA to perform hardware-accelerated machine learning tasks and deploy it on an unmanned multirotor aerial system (drone). 

The drone should be piloted manually within the line-of-sight (LOS) of the pilot, using a ground-to-air transmitter (TX) in the form of an off-the-shelf radio controller and receiver combination. The drone should feature a flight controller capable of self-stabilization using well-tuned PIDs and,
with the payload attached, the drone's flight duration should be at least 50\% of that without the payload (typically 10--15 minutes).
The total takeoff mass of the integrated system should not exceed 25 kilograms --- as specified by Transport Canada, as a pilot with a \textit{Basic} or \textit{Advanced Operations} certificate cannot operate a drone heavier than 25kg.

The machine learning model should analyze the video stream from the on-board camera and detect pedestrians within the frame with near-humanlike accuracy. The model should exploit the FPGA's inherent parallelism to speed up this analysis, accelerating key ML tasks such as matrix multiplication and convolution. The model should output results indicating where in the frame pedestrians have been detected, with this prediction having a tolerable update latency of one second or less. In addition, the base station should display \textit{bounding boxes} surrounding the model's pedestrians predictions, overlaying a video of reasonable quality (at least 640x480, 10fps). 
